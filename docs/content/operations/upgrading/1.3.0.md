---
title: 1.3.0+ Upgrade Notice
weight: 2
description: Migrating from Gloo 1.2.x+ to Gloo Enterprise 1.3.0 (~Gloo open-source 1.3.17) 
---

Upgrade using helm to the latest Gloo release, in-place with no downtime! As there were some important helm chart fixes
related to helm hooks between Gloo 1.2.x and Gloo 1.3.x, there are some one-time steps you may need to take in addition
`helm upgrade` to get your helm release into the proper state. This document outlines all the required and recommended steps.

### Upgrade Prereqs

Install:
 - [helm](https://github.com/helm/helm)
 - [hey](https://github.com/rakyll/hey) (optional, used to confirm upgrade was zero-downtime)
 - `glooctl` version 1.2.5+ (optional, used to confirm upgrade was zero-downtime)

Without `glooctl` 1.2.5+, `glooctl proxy url` will output the version mistmatch warning on stdout during upgrade, and our
`curl` and `hey` commands will fail.

This upgrade guide also requires that was gloo installed via `helm` or with `glooctl` version 1.2.0+
(i.e., gloo is a helm release named "gloo", you can confirm this exists with `helm ls --all-namespaces`). If upgrading
from an older version of Gloo, follow the steps outlined
[here]({{< versioned_link_path fromRoot="/operations/upgrading/1.0.0#example-upgrade-process" >}}) for a blue-green deployment
and cutover.

### Recommended Settings

If upgrading to Gloo 1.3.15 or lower, or to one of the Gloo Enterprise 1.3.0-beta releases, set the EDS warming timeout
to a nonzero value (recommended 5 minutes):
```shell script
kubectl patch settings -n gloo-system default \
    --patch '{"spec": {"gloo": {"endpointsWarmingTimeout": "5m"}}}' --type=merge
```

If gloo is not running in kubernetes and using the kubernetes load-balancer, then properly configure 
[health checks]({{< versioned_link_path fromRoot="/guides/traffic_management/request_processing/health_checks" >}})
on envoy and configure your load-balancer to leverage these health checks, so requests stop going to envoy once it
begins draining connections.

If gloo is running in kubernetes and using the kubernetes load-balancer, enable envoy readiness and liveness probes 
during the upgrade. This will instruct kubernetes to send requests exclusively to the healthy envoy during upgrade,
preventing potential downtime. The probes are not enabled in default installations because they can provide for a poor
getting started experience. The following example upgrade assumes you're running in kubernetes with the kubernetes
load-balancer.

### Example: Test a Zero-Downtime `helm upgrade`

You can skip straight to the upgrade steps [here](#upgrading).

##### Setup

Install gloo:
```shell script
glooctl install gateway --version 1.2.0 # `--version` flag requires glooctl 1.3.0+
```

Add an upstream:
```shell script
apiVersion: gloo.solo.io/v1
kind: Upstream
metadata:
  name: json-upstream
  namespace: gloo-system
spec:
  static:
    hosts:
      - addr: jsonplaceholder.typicode.com
        port: 80
```

And make it routable:
```shell script
apiVersion: gateway.solo.io/v1
kind: VirtualService
metadata:
  name: test-prefix
  namespace: gloo-system
spec:
  virtualHost:
    routes:
      - matchers:
         - prefix: /posts
        routeAction:
          single:
            upstream:
              name: json-upstream
              namespace: gloo-system
        options:
          autoHostRewrite: true
```

Wait until the following returns:
```shell script
curl $(glooctl proxy url)/posts
```

```shell script
  ... # omitted for brevity
  {
    "userId": 10,
    "id": 100,
    "title": "at nam consequatur ea labore ea harum",
    "body": "cupiditate quo est a modi nesciunt soluta\nipsa voluptas error itaque dicta in\nautem qui minus magnam et distinctio eum\naccusamus ratione error aut"
  }
]
```

##### Upgrading

Now Gloo is installed and ready for upgrade.

If running Gloo 1.2.x or lower (open-source or enterprise):
```shell script
glooctl debug yaml > gloo-state-backup.yaml                   # backup your state of the world
kubectl scale --replicas 0 deployment/gateway -n gloo-system  # so gateway doesn't update the proxy after we delete the gateway
kubectl scale --replicas 0 deployment/gloo -n gloo-system          # so gloo doesn't recreate the settings
kubectl scale --replicas 0 deployment/discovery -n gloo-system     # so discovery doesn't recreate the settings
kubectl scale --replicas 0 deployment/observability -n gloo-system # so observability doesn't recreate the settings
kubectl delete settings -n gloo-system --all # delete the helm-hook settings, so we can upgrade to settings managed by helm
kubectl delete gateway -n gloo-system --all  # delete the helm-hook gateway, so we can upgrade to gateways managed by helm
```

Note that when upgrading, if you had any non-default `Gateway` or `Settings` configuration, you will want to make sure
the generated gateways and settings match your previously configured gateways and settings by comparing the output of
`helm template` with your `gloo-state-backup.yaml` before running `helm upgrade`.

To get the configuration to match, you will likely need to set the following helm values in addition to the ones
provided in the examples below. 

- `gatewayProxies.NAME.gatewaySettings.options`
- `gatewayProxies.NAME.gatewaySettings.customHttpGateway`
- `gatewayProxies.NAME.gatewaySettings.customHttpsGateway`

{{% notice warning %}}
You will need to prefix these values with `gloo` if upgrading to Gloo Enterprise.
{{% /notice %}}

As well as your `Settings` values, such as the following, which don't need the `gloo` prefix:

- `settings.watchNamespaces`

A full list of supported helm chart values in open-source gloo can be found [here]({{< versioned_link_path fromRoot="/reference/helm_chart_values/" >}}). 
The enterprise-only list is [here]({{% versioned_link_path fromRoot="/installation/enterprise/#list-of-gloo-helm-chart-values" %}}).

<details><summary>Upgrade to Open-Source</summary>

Upgrade to open-source Gloo 1.3.x (helm 2 or helm 3):
{{< tabs >}}
{{< tab name="1min hey" codelang="shell">}}
hey -n 6000 -c 10 -q 10 $(glooctl proxy url)/posts & helm upgrade gloo gloo/gloo --namespace gloo-system --version 1.3.17 \
    --set gatewayProxies.gatewayProxy.podTemplate.probes=true
{{< /tab >}}
{{< tab name="helm upgrade only" codelang="shell">}}
helm upgrade gloo gloo/gloo --namespace gloo-system --version 1.3.17 \
    --set gatewayProxies.gatewayProxy.podTemplate.probes=true
{{< /tab >}}
{{< /tabs >}}

</details>

<details><summary>Upgrade to Gloo Enterprise</summary>

If upgrading from Gloo Enterprise 1.3.0-beta6 or lower (including 1.2.x) to Gloo Enterprise 1.3.0 (or the beta7), you
will also need to delete the grafana deployment and service to work around breaking changes in the grafana subchart:
```
kubectl delete deployment -n gloo-system glooe-grafana
kubectl delete service -n gloo-system glooe-grafana
```

Upgrade to Gloo Enterprise 1.3.0 (helm 2 or helm 3):

{{% notice note %}}
The `--set grafana.persistence.storageClassName=<currently installed pvc storage class>` is only required if upgrading
from Gloo Enterprise 1.3.0-beta6 or lower and grafana is enabled (the default). In most installations the storage class
will be `standard`, but `gp2` is common for EKS and it's worth confirming the deployed storage class before attempting
installation with `kubectl get pvc -n gloo-system`.
{{% /notice %}}

{{< tabs >}}
{{< tab name="1min hey" codelang="shell">}}
hey -n 6000 -c 10 -q 10 $(glooctl proxy url)/posts & helm upgrade gloo glooe/gloo-ee --namespace gloo-system --version=1.3.0 \
    --set license_key=$LICENSE_KEY \
    --set gloo.gatewayProxies.gatewayProxy.podTemplate.probes=true \
    --set grafana.persistence.storageClassName=standard # if required, storage class must match the PVC that's already deployed
{{< /tab >}}
{{< tab name="helm upgrade only" codelang="shell">}}
helm upgrade gloo glooe/gloo-ee --namespace gloo-system --version=1.3.0 \
    --set license_key=$LICENSE_KEY \
    --set gloo.gatewayProxies.gatewayProxy.podTemplate.probes=true \
    --set grafana.persistence.storageClassName=standard # if required, storage class must match the PVC that's already deployed
{{< /tab >}}
{{< /tabs >}}

{{% notice note %}}
If your initial version was Gloo Enterprise 1.3.0-beta6 or lower, you will always need to provide the 
`grafana.persistence.storageClassName` value going forward for upgrades, so add that to your helm values.
{{% /notice %}}

</details>

Wait for the request distribution at the end to confirm that requests weren't dropped. You should see a response
similar to the following (note that all requests get http 200):
```shell script
[1] 20563
Release "gloo" has been upgraded. Happy Helming!
NAME: gloo
LAST DEPLOYED: Tue Mar 31 09:02:16 2020
NAMESPACE: gloo-system
STATUS: deployed
REVISION: 6
TEST SUITE: None
➜  gloo git:(doc_upgrade) (⎈ minikube:default)
Summary:
  Total:	17.8115 secs
  Slowest:	1.0715 secs
  Fastest:	0.0179 secs
  Average:	0.1624 secs
  Requests/sec:	280.7173


Response time histogram:
  0.018 [1]	|
  0.123 [1892]	|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
  0.229 [2411]	|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
  0.334 [493]	|■■■■■■■■
  0.439 [121]	|■■
  0.545 [31]	|■
  0.650 [2]	|
  0.755 [12]	|
  0.861 [2]	|
  0.966 [8]	|
  1.072 [27]	|


Latency distribution:
  10% in 0.0730 secs
  25% in 0.1033 secs
  50% in 0.1438 secs
  75% in 0.1958 secs
  90% in 0.2452 secs
  95% in 0.3176 secs
  99% in 0.6126 secs

Details (average, fastest, slowest):
  DNS+dialup:	0.0000 secs, 0.0179 secs, 1.0715 secs
  DNS-lookup:	0.0000 secs, 0.0000 secs, 0.0000 secs
  req write:	0.0000 secs, 0.0000 secs, 0.0005 secs
  resp wait:	0.1587 secs, 0.0167 secs, 1.0713 secs
  resp read:	0.0036 secs, 0.0002 secs, 0.2522 secs

Status code distribution:
  [200]	5000 responses




[1]  + 20563 done       hey -n 5000 $(glooctl proxy url)/posts
```

Run `glooctl check` to ensure everything is healthy:
```shell script
glooctl check
```